{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a6f51fe7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langgraph.graph import StateGraph, START,END\n",
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "from dotenv import load_dotenv\n",
    "from typing import TypedDict,Literal\n",
    "from pydantic import BaseModel,Field"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a2e22893",
   "metadata": {},
   "outputs": [],
   "source": [
    "load_dotenv()\n",
    "llm_model = ChatGoogleGenerativeAI(\n",
    "    model = \"gemini-2.5-flash\",\n",
    "    temperature = 0.7\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "95ed72f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SentimentSchema(BaseModel):\n",
    "    sentiment : Literal[\"positive\",\"negative\"] = Field(description=\"Sentiment of the review\")\n",
    "    \n",
    "class DiagnosisSchema(BaseModel):\n",
    "    issue_type : Literal['UX','Performance','Bug','Support','Other'] = Field(description=\"Yhe category of the issue mentioned in the review\")\n",
    "    tone : Literal['Angry','Frustrated','Disappointed','calm'] = Field(description='Yhe emotional toned express by the user')\n",
    "    urgency : Literal[\"low\",'medium','high'] = Field(description=\"How urgent or ctitical the issue appears to be\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6b048b95",
   "metadata": {},
   "outputs": [],
   "source": [
    "structured_model = llm_model.with_structured_output(SentimentSchema)\n",
    "structured_model2 = llm_model.with_structured_output(DiagnosisSchema)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "35024acb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'positive'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt = \"What is the sentiment of the following review - The software too good\"\n",
    "structured_model.invoke(prompt).sentiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "99ba8877",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Define State\n",
    "\n",
    "class ReviewState(TypedDict):\n",
    "    review : str\n",
    "    sentiment : Literal[\"positive\",\"negative\"]\n",
    "    diagnosis : dict\n",
    "    response : str"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "66c99fa7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_sentiment(state : ReviewState):\n",
    "    \n",
    "    prompt = f\"For the following review find out the sentiment \\n{state['review']}\"\n",
    "    sentiment = structured_model.invoke(prompt).sentiment\n",
    "    return {\"sentiment\": sentiment}\n",
    "\n",
    "\n",
    "def check_sentiment(state : ReviewState) -> Literal[\"positive_response\",\"run_diagnosis\"]:\n",
    "    if state[\"sentiment\"] == \"positive\":\n",
    "        return \"positive_response\"\n",
    "    else:\n",
    "        return \"run_diagnosis\"\n",
    "    \n",
    "def positive_response(state:ReviewState):\n",
    "    prompt = f\"\"\" Write a warm thank-you message in response to this review :\n",
    "    \\n\\n\"{state['review']}\\n\"\\n\n",
    "    Also kindly ask the user to leave feedback on our website.\"\"\"\n",
    "    \n",
    "    \n",
    "    response = llm_model.invoke(prompt).content\n",
    "    return {\"response\":response }\n",
    "\n",
    "def run_diagnosis(state:ReviewState):\n",
    "    prompt = f'Diagnose this negative review : \\n\\n{state['review']}\\n'\n",
    "    \n",
    "    response = structured_model2.invoke(prompt)\n",
    "    return{'diagnosis': response.model_dump()}\n",
    "    \n",
    "\n",
    "def negative_response(state:ReviewState):\n",
    "    diagnosis = state['diagnosis']\n",
    "    prompt = f\"\"\" You are a support assistant.\n",
    "    The user had a '{diagnosis['issue_type']}' issue, sounded '{diagnosis['tone']}', and marked urgency as '{diagnosis['urgency']}',\n",
    "    Write an empathetic , helpful resulution message.\n",
    "    \"\"\"\n",
    "    \n",
    "    response = llm_model.invoke(prompt).content\n",
    "    return {'response': response}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "4d67336b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define graph\n",
    "\n",
    "graph = StateGraph(ReviewState)\n",
    "graph.add_node(\"find_sentiment\",find_sentiment)\n",
    "graph.add_node(\"positive_response\",positive_response)\n",
    "graph.add_node(\"run_diagnosis\",run_diagnosis)\n",
    "graph.add_node(\"negative_response\",negative_response)\n",
    "\n",
    "## add edges\n",
    "graph.add_edge(START,'find_sentiment')\n",
    "graph.add_conditional_edges('find_sentiment',check_sentiment)\n",
    "graph.add_edge('positive_response',END)\n",
    "graph.add_edge('run_diagnosis','negative_response')\n",
    "graph.add_edge('negative_response',END)\n",
    "\n",
    "workflow = graph.compile()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "ed13625d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'review': 'Disappointed with the product. It did not perform as expected, the quality feels average, and it is not worth the price. There are better alternatives available.',\n",
       " 'sentiment': 'negative',\n",
       " 'diagnosis': {'issue_type': 'Other',\n",
       "  'tone': 'Disappointed',\n",
       "  'urgency': 'medium'},\n",
       " 'response': \"I'm genuinely sorry to hear that you're feeling disappointed. It's completely understandable to feel that way when things aren't working as expected or when you encounter an unexpected issue. Please know that even though your issue is categorized as 'Other', it's no less important, and my top priority is to help you.\\n\\nI see this is marked with medium urgency, and I assure you I'll be giving it my full attention to find a helpful solution as quickly as possible.\\n\\nTo help me understand exactly what happened and how it's impacting you, could you please provide a few more details? For example:\\n\\n*   What specifically occurred?\\n*   When did you first notice the problem?\\n*   Are there any error messages, specific circumstances, or steps that lead to the issue?\\n\\nOnce I have a clearer picture, I'll work diligently to either resolve it directly, provide a workaround, or connect you with the right resources to get things back on track.\\n\\nPlease reply to this message with the information, and I'll get started right away.\\n\\nThank you for your patience, and I'm here to help.\"}"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "initial_state = {\n",
    "    'review': \"Disappointed with the product. It did not perform as expected, the quality feels average, and it is not worth the price. There are better alternatives available.\"\n",
    "}\n",
    "\n",
    "workflow.invoke(initial_state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bdc62d8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv (3.13.3)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
